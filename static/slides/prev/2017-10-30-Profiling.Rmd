---
layout: page
title: Benchmarking & Parallelization
reading: "<a href='http://adv-r.had.co.nz/Profiling.html'>Adv. R</a>, <a href='https://rstudio.github.io/profvis/'>Profvis</a>, <br/> <a href='https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf'>parallel</a>, <a href='http://cran.r-project.org/web/packages/doMC/vignettes/gettingstartedMC.pdf'>doMC and foreach</a>"
notes: ""
output: 
    ioslides_presentation:
        widescreen: true
slides: true
link: true
---


```{r set-options, echo=FALSE}
options(width = 90)
suppressMessages(library(dplyr))
knitr::opts_chunk$set(fig.align="center", warning=FALSE, message=FALSE)
```

# Profiling & Benchmarking


## Profiling & Benchmarking

* Improved performance comes from iteration, and learning the most common pitfalls

* Don't sweat the small stuff - Coder time vs Run time vs Compute costs

* Measure it, or it didn't happen

* "Premature optimization is the root of all evil (or at least most of it) in programming." -Knuth


## How do we measure?

Simplest tool is R's base `system.time` which can be used to wrap any other call or calls.

```{r}
system.time({rnorm(1e6)})
system.time({rnorm(1e4) %*% t(rnorm(1e4))})
``` 


## Better benchmarking (pt. 1) {.smaller}

We can do better (better precision) using the microbenchmark package 

```{r, eval=FALSE}
install.packages("microbenchmark")
```
```{r}
library(microbenchmark)

d = abs(rnorm(1000))
r = microbenchmark(
      exp(log(d)/2),
      d^0.5,
      sqrt(d),
      times = 1000
    )
print(r)
```


##

```{r}
boxplot(r)
```


## Better benchmarking (pt. 2) {.smaller}

We can also do better using the rbenchmark package 

```{r, eval=FALSE}
install.packages("rbenchmark")
```
```{r}
library(rbenchmark)

d = abs(rnorm(1000))
benchmark(
  exp(log(d)/2),
  d^0.5,
  sqrt(d),
  replications = 1000,
  order = "relative"
)
```


## Example 1 {.smaller}

Earlier we mentioned that growing a vector as you collect results is bad, just how bad is it? Benchmark the following three functions and compare their performance.


```{r, eval=FALSE}
good = function() {
    res = rep(NA, 1e4)
    for(i in seq_along(res))
        res[i] = sqrt(i)
}
```


```{r, eval=FALSE}
bad = function() {
    res = numeric()
    for(i in 1:1e4)
        res = c(res,sqrt(i))

}
```

```{r, eval=FALSE}
best = function() {
    sqrt(1:1e4)
}
```


## Example 2 {.smaller}

Lets compare looping vs. the map (apply) function vs dplyr.

* First we will construct a large data frame

```{r, eval=FALSE}
set.seed(523)
d = data_frame(
  A = rnorm(1e5),
  B = runif(1e5),
  C = rexp(1e5),
  D = sample(letters, 1e5, replace=TRUE)               
)
```

* Implement functions that will return a data frame containing the maximum value of each *column*
    
    * A `map` function
    * A single `for` loop
    * dplyr

* Benchmark all of your preceding functions using data frame `d`, which is the fastest, why do you think this is the case? 10 replicates per function is sufficient.

* What would happen if `d` was smaller? (e.g. take only the first 1000 rows)



```{r echo=FALSE, message=FALSE}
library(parallel)
library(doMC)
library(foreach)
library(dplyr)
library(ggplot2)
library(purrr)
library(tidyr)
```

# Parallelization

## `parallel`

Part of the base packages in R 

* tools for the forking of R processes (some functions do not work on Windows)

* Core functions:
    
    * `detectCores`

    * `pvec`

    * `mclapply`

    * `mcparallel` & `mccollect`


## detectCores

Surprisingly, detects the number of cores of the current system.

```{r eval=FALSE}
detectCores()

## [1] 24
```

## pvec

Parallelization of a vectorized function call

```{r eval=FALSE}
system.time(pvec(1:1e7, sqrt, mc.cores = 1))

##   user  system elapsed 
##  0.214   0.029   0.243 

system.time(pvec(1:1e7, sqrt, mc.cores = 4))

##   user  system elapsed 
##  0.442   0.185   0.631 

system.time(pvec(1:1e7, sqrt, mc.cores = 8))

##   user  system elapsed 
##  0.532   0.389   0.372 
```


##

```{r eval=FALSE}
cores = c(1,2,4,8,16)
order = 6:8
res = map(
  cores, 
  function(x) {
     map_dbl(order, function(y) system.time(pvec(1:(10^y), sqrt, mc.cores=x))[3]) 
  }
) %>% do.call(rbind,.)
rownames(res) = paste0(cores," cores")
colnames(res) = paste0("10^",order)
res

##            10^6  10^7  10^8
##  1 cores  0.016 0.282 1.489
##  2 cores  0.070 0.526 5.198
##  4 cores  0.052 0.430 5.023
##  8 cores  0.054 0.376 4.098
##  16 cores 0.073 0.401 4.049
```

##

```{r include=FALSE}
load("../data/pvec_res.Rdata")
```


```{r echo=FALSE}
d = as_data_frame(res) %>%
  mutate(cpus = rownames(res)) %>%
  gather(size, time, -cpus) %>%
  mutate(size = eval(expression(size)))

ggplot(d, aes(y=time, x=size, color=cpus)) + geom_point(alpha=0.75, size=2) + scale_y_log10()
```


## mclapply {.smaller}

Parallelized version of `lapply`

```{r eval=FALSE}
system.time(rnorm(1e6))

##   user  system elapsed 
##  0.101   0.007   0.107 

system.time(unlist(mclapply(1:10, function(x) rnorm(1e5), mc.cores = 2)))

##   user  system elapsed 
##  0.148   0.136   0.106 

system.time(unlist(mclapply(1:10, function(x) rnorm(1e5), mc.cores = 4)))

##   user  system elapsed 
##  0.242   0.061   0.052 ```
```


## {.smaller}

```{r eval=FALSE}
system.time(unlist(mclapply(1:10, function(x) rnorm(1e5), mc.cores = 4)))

##   user  system elapsed 
##  0.097   0.047   0.079 

system.time(unlist(mclapply(1:10, function(x) rnorm(1e5), mc.cores = 8)))

##   user  system elapsed 
##  0.193   0.076   0.040 

system.time(unlist(mclapply(1:10, function(x) rnorm(1e5), mc.cores = 10)))

##   user  system elapsed 
##  0.162   0.083   0.041 

system.time(unlist(mclapply(1:10, function(x) rnorm(1e5), mc.cores = 12)))

##   user  system elapsed 
##  0.098   0.065   0.037 
```


## mcparallel {.smaller}

Asynchronously evaluation of an R expression in a separate process

```{r}
m = mcparallel(rnorm(1e6))
n = mcparallel(rbeta(1e6,1,1))
o = mcparallel(rgamma(1e6,1,1))

str(m)
str(n)
```


## mccollect

Checks `mcparallel` objects for completion

```{r}
str(mccollect(list(m,n,o)))
```


## mccollect - waiting {.smaller}

```{r}
p = mcparallel(mean(rnorm(1e5)))
mccollect(p, wait = FALSE, 10) # will retrieve the result (since it's fast)
mccollect(p, wait = FALSE)     # will signal the job as terminating
mccollect(p, wait = FALSE)     # there is no longer such a job
```


# doMC & foreach

## doMC & foreach

Packages by Revolution Analytics that provides the `foreach` function which is a parallelizable `for` loop (and then some).

* Core functions:
    
    * `registerDoMC`

    * `foreach`, `%dopar%`, `%do%`


## registerDoMC {.smaller}

Primarily used to set the number of cores used by `foreach`, by default uses `options("cores")` or half the number of cores found by `detectCores` from the parallel package.

```{r eval=FALSE}
options("cores")

## $cores
## NULL

detectCores()

## [1] 24

getDoParWorkers()

## [1] 1

registerDoMC(4)
getDoParWorkers()

## [1] 4
```


## foreach {.smaller}

A slightly more powerful version of base `for` loops (think `for` with an `lapply` flavor). Combined with `%do%` or `%dopar%` for single or multicore execution.

```{r}
for(i in 1:10) sqrt(i)

foreach(i = 1:5) %do% sqrt(i)   
```


## foreach - iterators {.smaller}

`foreach` can iterate across more than one value, but it doesn't do length coercion

<div class="columns-2">
```{r}
foreach(i = 1:5, j = 1:5) %do% sqrt(i^2+j^2)   
```
```{r}
foreach(i = 1:5, j = 1:2) %do% sqrt(i^2+j^2)   
```
<br/><br/><br/><br/><br/><br/>
</div>


## foreach - combining results {.smaller}

```{r}
foreach(i = 1:5, .combine='c') %do% sqrt(i)   
foreach(i = 1:5, .combine='cbind') %do% sqrt(i)   
foreach(i = 1:5, .combine='+') %do% sqrt(i)   
```


## foreach - parallelization {.smaller}

Swapping out `%do%` for `%dopar%` will use the parallel backend.

```{r}
registerDoMC(4)
system.time(foreach(i = 1:10) %dopar% mean(rnorm(1e6)))
registerDoMC(8)
system.time(foreach(i = 1:10) %dopar% mean(rnorm(1e6)))
registerDoMC(12)
system.time(foreach(i = 1:10) %dopar% mean(rnorm(1e6)))
```


## Example 3 - Bootstraping {.smaller}

Bootstrapping is a resampling scheme where the original data is repeatedly reconstructed by taking a sample (with replacement) of the same size as the original data, and using that to conduct whatever analysis procedure is of interest. Below is an example of fitting a local regression (`loess`) to some synthetic data, we will construct a bootstrap prediction interval for this model.


```{r}
set.seed(3212016)
d = data.frame(x = 1:120) %>%
    mutate(y = sin(2*pi*x/120) + runif(length(x),-1,1))

l = loess(y ~ x, data=d)
d = d %>% mutate(
  pred_y = predict(l),
  pred_y_se = predict(l,se=TRUE)$se.fit
) %>% mutate(
  pred_low  = pred_y - 1.96 * pred_y_se,
  pred_high = pred_y + 1.96 * pred_y_se
)
```

```{r eval=FALSE}
ggplot(d, aes(x,y)) +
  geom_point(color="darkgrey") +
  geom_ribbon(aes(ymin=pred_low, ymax=pred_high), fill="red", alpha=0.25) +
  geom_line(aes(y=pred_y)) +
  theme_bw()
```

##

```{r echo=FALSE, fig.align="center", fig.width=9, fig.height=6}
ggplot(d, aes(x,y)) +
  geom_point(color="gray50") +
  geom_ribbon(aes(ymin=pred_low, ymax=pred_high), fill="red", alpha=0.25) +
  geom_line(aes(y=pred_y)) +
  theme_bw()
```

## Example 3 - Cont. {.smaller}

We will now re-implement the code below using one of the parallelization techniques we have just discussed and will then check the performance with 1, 2, and 4 cores.


```{r eval=FALSE}
n_rep = 5000
d_xy = select(d, x, y)

res = map(1:n_rep, function(i) {
  d_xy %>% 
    select(x,y) %>% 
    sample_n(nrow(d), replace=TRUE) %>%
    loess(y ~ x, data=.) %>%
    predict(newdata=d) %>%
    setNames(NULL)
}) %>% do.call(cbind, .)

d = d %>% mutate(
  bs_low = apply(res,1,quantile,probs=c(0.025), na.rm=TRUE),
  bs_high  = apply(res,1,quantile,probs=c(0.975), na.rm=TRUE)
)

ggplot(d, aes(x,y)) +
  geom_point(color="gray50") +
  geom_ribbon(aes(ymin=pred_low, ymax=pred_high), fill="red", alpha=0.25) +
  geom_ribbon(aes(ymin=bs_low, ymax=bs_high), fill="blue", alpha=0.25) +
  geom_line(aes(y=pred_y)) +
  theme_bw()
```


## 

<div class="centered">
<img src="imgs/bootstrap_loess.png" style="width:950px">
</div>



## What to use when?

Optimal use of multiple cores is hard, there isn't one best solution

* Don't underestimate the overhead cost

* More art than science - experimentation is key

* Measure it or it didn't happen

* Be aware of the trade off between developer time and run time



# BLAS and LAPACK

## Statistics and Linear Algebra

An awful lot of statistics is at its core linear algebra.

<br/>

For example:

* Linear regession models, find

$$ \hat{\beta} = (X^T X)^{-1} X^Ty $$

* Principle component analysis

    * Find $T = XW$ where $W$ is a matrix whose columns are the eigenvectors of $X^TX$.
    
    * Often solved via SVD - Let $X = U\Sigma W^T$ then $T = U\Sigma$.


## Numerical Linear Algebra

Not unique to Statistics, these are the type of problems that come up across all areas of numerical computing.

* Numerical linear algebra $\ne$ mathematical linear algebra

<br/>

* Efficiency and stability of numerical algorithms matter

    * Designing and implementing these algorithms is hard

<br/>

* Don't reinvent the wheel - common core linear algebra tools (well defined API)



## BLAS and LAPACK {.smaller}

Low level algorithms for common linear algebra operations

<br/>

BLAS

* **B**asic **L**inear **A**lgebra **S**ubprograms

* Copying, scaling, multiplying vectors and matrices

* Origins go back to 1979, written in Fortran

<br/>

LAPACK

* **L**inear **A**lgebra **Pack**age

* Higher level functionality building on BLAS.

* Linear solvers, eigenvalues, and matrix decompositions

* Origins go back to 1992, mostly Fortran (expanded on LINPACK, EISPACK)


## Modern variants?

Most default BLAS and LAPACK implementations (like R's defaults) are somewhat dated

* Written in Fortran and designed for a single cpu core  

* Certain (potentially non-optimal) hard coded defaults (e.g. block size).

<br/>

Multithreaded alternatives:

* ATLAS - Automatically Tuned Linear Algebra Software

* OpenBLAS - fork of GotoBLAS from TACC at UTexas

* Intel MKL - Math Kernel Library, part of Intel's commercial compiler tools

* cuBLAS / Magma - GPU libraries from NVidia and UTK respectively

<!--
## Naming conventions

BLAS and LAPACK subroutines are named using form `pmmaaa` where:

* `p` is a one letter code for the type of data

    * `S` single precision floating point
    * `D` double precision floating point
    * `C` complex single precision floating point
    * `Z` complex double precision floating point

* `mm` is a two letter code for the type of matrix expected by the subroutine

* `aaa` is a one to three letter code denoting the algorithm implemented by subroutine


## BLAS Example - DGEMM{.smaller}

`D` - type double, `GE` - general matrix, `MM` - matrix / matrix multiplication.

```
dgemm(   character   TRANSA,
         character   TRANSB,
         integer     M,
         integer     N,
         integer     K,
         double precision    ALPHA,
         double precision, dimension(lda,*)  A,
         integer     LDA,
         double precision, dimension(ldb,*)  B,
         integer     LDB,
         double precision    BETA,
         double precision, dimension(ldc,*)  C,
         integer     LDC 
     )   
```

`DGEMM` performs one of the matrix-matrix operations

$$C = \alpha op( A ) \times op( B ) + \beta C$$

where $op( X )$ is either $op( X ) = X$ or $op( X ) = X^T$, $\alpha$ and $\beta$ are scalars, and $A$, $B$ and $C$ are matrices, with $op( A )$
an $m$ by $k$ matrix, $op( B )$  a $k$ by $n$ matrix and $C$ an $m$ by $n$ matrix.


## LAPACK Example - `DPOTRF` {.smaller}

`D` - type double, `PO` - positive definite matrix, `TRF` - triangular factorization


```
dpotrf(  character   UPLO,
         integer     N,
         double precision, dimension( lda, * )   A,
         integer     LDA,
         integer     INFO 
      )   
```

`DPOTRF` computes the Cholesky factorization of a real symmetric positive definite matrix $A$.

The factorization has the form
$$A = U^T * U,  \text{if UPLO = 'U', or}$$
$$A = L  * L^T,  \text{if UPLO = 'L',}$$
where $U$ is an upper triangular matrix and $L$ is lower triangular.
-->

## OpenBLAS DGEMM (Matrix Multiply) Performance {.smaller}

```{r, eval=FALSE, include=FALSE}
library(RhpcBLASctl)
x=matrix(runif(5000^2),ncol=5000)

sizes = c(100,500,1000,2000,3000,4000,5000)
cores = c(1,2,4,8)

sapply(
  cores, 
  function(n_cores) 
  {
    blas_set_num_threads(n_cores)
    sapply(
      sizes, 
      function(s) 
      {
           y = x[1:s,1:s]
           system.time(y %*% y)[3]
      }
    )
  }
)
```

|  n   | 1 core | 2 cores | 4 cores | 8 cores |
|------|--------|---------|---------|---------|
| 100  |  0.001 | 0.001   | 0.000   | 0.000   |
| 500  |  0.018 | 0.011   | 0.008   | 0.008   |
| 1000 |  0.128 | 0.068   | 0.041   | 0.036   |
| 2000 |  0.930 | 0.491   | 0.276   | 0.162   |
| 3000 |  3.112 | 1.604   | 0.897   | 0.489   |
| 4000 |  7.330 | 3.732   | 1.973   | 1.188   |
| 5000 | 14.223 | 7.341   | 3.856   | 2.310   |

##

```{r echo=FALSE}
tribble(
  ~"n",  ~"1 core", ~"2 cores", ~"4 cores", ~"8 cores",
  100,  0.001,  0.001, 0.000, 0.000,
  500,  0.018,  0.011, 0.008, 0.008,
  1000, 0.128,  0.068, 0.041, 0.036,
  2000, 0.930,  0.491, 0.276, 0.162,
  3000, 3.112,  1.604, 0.897, 0.489,
  4000, 7.330,  3.732, 1.973, 1.188,
  5000, 14.223, 7.341, 3.856, 2.310
) %>% 
  gather(cores,time,-n) %>%
  ggplot(aes(x=n, y=time, color=cores)) +
    geom_point()
```



## GPU vs OpenBLAS (NVidia K20X - Kepler) {.smaller}

|  M   |   N   |   K   | MAGMA (ms) | cuBLAS (ms) | CPU (ms)   |
|------|-------|-------|------------|-------------|------------|
| 1088 |  1088 |  1088 |     4.68   |     5.98    |     69.54  |
| 2112 |  2112 |  2112 |    29.79   |    17.33    |    329.42  |
| 3136 |  3136 |  3136 |    98.68   |    54.17    |    958.82  |
| 4160 |  4160 |  4160 |   230.35   |   125.54    |   2119.71  |
| 5184 |  5184 |  5184 |   448.66   |   240.91    |   3998.74  |
| 6208 |  6208 |  6208 |   772.88   |   412.61    |   6801.51  |
| 7232 |  7232 |  7232 |  1224.33   |   650.76    |  10629.33  |
| 8256 |  8256 |  8256 |  1823.29   |   967.20    |  15740.96  |
| 9280 |  9280 |  9280 |  2590.17   |  1370.57    |  22367.01  |
|10304 | 10304 | 10304 |  3517.43   |  1872.64    |  30597.85  |


## GPU vs OpenBLAS (NVidia P100 - Pascal) {.smaller}

|  M   |   N   |   K   | MAGMA (ms)  | cuBLAS (ms) | CPU (ms)  |
|------|-------|-------|-------------|-------------|-----------|
| 1088 |  1088 |  1088 |    0.94     |    0.72     |    64.82  |
| 2112 |  2112 |  2112 |    5.60     |    4.71     |   154.75  |
| 3136 |  3136 |  3136 |   17.71     |   13.82     |   357.92  |
| 4160 |  4160 |  4160 |   41.45     |   32.43     |   741.42  |
| 5184 |  5184 |  5184 |   80.21     |   62.28     |  1293.62  |
| 6208 |  6208 |  6208 |  138.34     |  106.78     |  2223.03  |
| 7232 |  7232 |  7232 |  218.60     |  167.36     |  3413.69  |
| 8256 |  8256 |  8256 |  326.42     |  249.71     |  4892.46  |
| 9280 |  9280 |  9280 |  465.48     |  353.87     |  6999.95  |
|10304 | 10304 | 10304 |  638.43     |  483.86     |  9547.43  |
